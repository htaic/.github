name: Release docker package

on:
  workflow_call:
    inputs:
      service_name:
        required: true
        description: "Name of helm controlled service to release"
        type: string
      ASSUME_ROLE:
        required: true
        description: "AWS arn to assume for execution"
        type: string
      working_dir:
        required: false
        description: "Operating path for path dependent steps"
        type: string
        default: .
      helm_values:
        required: false
        description: "Path of local helm values.yaml file"
        type: string
        default: DevValues.yaml
      helm_source:
        required: false
        description: "Helm URL to register packages"
        type: string
        default: https://htaic.github.io/helm-charts
      helm_chart:
        required: false
        description: "Name of helm package"
        type: string
        default: htaic/microfrontend
      cluster_name:
        required: false
        description: "Name of EKS cluster"
        type: string
        default: k8s-ht-dev-saas-uw2-1
    secrets:
      GH_MANAGEPACKAGETOKEN:
        required: true
      APPID:
        required: true
      APPSECRET:
        required: true
      REGION:
        required: true
      CODEFREEZE:
        required: true

env:
  CODEFREEZE: ${{ secrets.CODEFREEZE }}

jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.APPID }}
          aws-secret-access-key: ${{ secrets.APPSECRET }}
          aws-region: ${{ secrets.REGION }}
          #this piece isn't working, so I added an sts assume-role command in the next step
          # https://github.com/aws-actions/configure-aws-credentials/issues/465

      #https://stackoverflow.com/questions/63241009/aws-sts-assume-role-in-one-command
      # this assume role command ONLY WORKS if you do it at the begining of the step. If you do not include this in your step, the action's step is
      # not authenticated and will result in permission errors.
      - name: Authenticate and run Helm
        if: ${{ env.CODEFREEZE == 'false' }}
        working-directory: ${{ inputs.working_dir }}
        run: |
          export $(printf "AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s" \
          $(aws sts assume-role \
          --role-arn $ASSUME_ROLE \
          --role-session-name GitHubPipeline \
          --query "Credentials.[AccessKeyId,SecretAccessKey,SessionToken]" \
          --output text))
          aws eks update-kubeconfig --region $AWS_REGION --name $K8S_NAME
          helm repo add htaic $HELM_SOURCE && helm repo update
          deployment_count=$(kubectl get deployments -A | grep -ci "${HELM_CHART#*/}" || true)
          helm upgrade --install --atomic --namespace hanwha -f $VALUES_FILE $SERVICE_NAME $HELM_CHART
          if [[ "${deployment_count}" == "1" ]]; then kubectl patch deployment "${HELM_CHART#*/}" -n hanwha -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"date\":\"`date +'%s'`\"}}}}}"; fi
          kubectl get all --all-namespaces
        env:
          ASSUME_ROLE: ${{ inputs.ASSUME_ROLE }}
          AWS_ACCESS_KEY_ID: ${{ secrets.APPID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.APPSECRET }}
          AWS_REGION: ${{ secrets.REGION }}
          K8S_NAME: ${{ inputs.cluster_name }}
          SERVICE_NAME: ${{ inputs.service_name }}
          VALUES_FILE: ${{ inputs.helm_values }}
          HELM_SOURCE: ${{ inputs.helm_source }}
          HELM_CHART: ${{ inputs.helm_chart }}
  service-api-tests:
    needs: release
    uses: htaic/.github/.github/workflows/api-tests-service.yml@main
    with: 
      # TODO update environment name when there are more clusters
      environment_name: "dev"
      working_dir: ${{ inputs.working_dir }}
